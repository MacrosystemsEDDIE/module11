---
title: "Module 11 Canning data workflow"
author: "Mary Lofton, Austin Delany, Sherry Zhai, Kamilla Kurucz"
date: "2025-07-31"
output: md_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load packages (install any you don't have!)
```{r}
library(tidyverse)
library(lubridate)
library(zoo)
library(fable)
library(feasts)
library(urca)
library(tsibble)
library(scoringRules)
```

### Source function from Sherry
```{r}
source('fct_awss3Connect_V2.R')
```

### Download raw profile data
```{r}
profile_data_download <- awss3Connect(filename = 'data-warehouse/dbca/wiski/wiski_30072025_merged.csv')
```

### Data wrangling code for profile data from Austin
```{r}
sensor_codes <- read_csv("./sensorcode.csv")

# pulled BAC site per Matt's recommendation
# trying KEN as a second site
cannsite <- c('KEN')

profile_data <- profile_data_download %>%  
  dplyr::filter(`Program Site Ref` %in% cannsite) |> 
  select(site_ref = `Site Ref`, 
         program = `Program Site Ref`, 
         time = `Collect Time`, 
         date = `Collect Date`, 
         depth = `Sample Depth (m)`, 
         temperature = `Temperature (deg C)`, 
         oxygen = `O2-{DO conc} (mg/L)`,
         don = `N (sum sol org) {DON} (ug/L)`,
         chla = `Chlorophyll a (by vol) (mg/L)`,
         frp = `PO4-P (sol react) {SRP FRP} (ug/L)`,
         nit = `NO3-N (sol) (ug/L)`,
         amm = `NH3-N/NH4-N (sol) (ug/L)`) |> 
  mutate(time = format(strptime(time, "%I:%M:%S %p"), "%H:%M:%S")) |> # convert from AM/PM to 24-hour 
  mutate(datetime = paste(date, time))

profile_data_grouped <- profile_data |> 
  mutate(depth_rounded = plyr::round_any(depth, 0.25))  |> # bin depths by rounding -- matches depth configuration 
  select(-depth) |> 
  dplyr::rename(depth = depth_rounded) |> 
  dplyr::filter(!is.na(depth)) |> 
  pivot_longer(cols = temperature:amm, names_to = 'variable', values_to = 'data') |> 
  summarise(observation = mean(data, na.rm = TRUE), .by = c("datetime","variable","depth")) |> 
  mutate(datetime = lubridate::force_tz(lubridate::as_datetime(datetime, format = '%d/%m/%Y %H:%M:%S')), tzone = 'Australia/Perth') |>
  mutate(datetime = lubridate::with_tz(datetime, tzone = "UTC")) |> 
  mutate(datetime = lubridate::round_date(datetime, unit = 'hour')) |> 
  mutate(site_id = cannsite) |> #mutate(site_id = 'BAC') |> 
  select(datetime, site_id, depth, observation, variable) |> 
  mutate(date = as.Date(datetime)) |> 
  group_by(date, variable) |> 
  mutate(min_datetime = min(datetime)) |> 
  ungroup() |> 
  group_by(date, variable) |> 
  dplyr::filter(datetime == min_datetime) |> 
  ungroup() |> 
  dplyr::filter(!is.nan(observation)) |> 
  group_by(date, variable, site_id, depth) |>
  summarize(observation = mean(observation, na.rm = TRUE)) %>%
  ungroup() %>%
  select(datetime = date, site_id, depth, observation, variable) %>%
  arrange(datetime, variable, depth)

```

### Plot wrangled profile data
```{r}
plot_dat <- profile_data_grouped %>%
  filter(year(datetime) %in% c(2020:2024))

ggplot(data = plot_dat) +
  geom_point(aes(x = datetime, y = observation, group = as.factor(depth), color = as.factor(depth)))+
  facet_wrap(facets = vars(variable), scales = "free")
```

### Get dates of chlorophyll-a observations, then identify closest-in-time observations for other variables and join into one dataframe
```{r}
chla_dates <- profile_data_grouped %>%
  filter(variable == "chla") %>%
  group_by(datetime) %>%
  filter(depth == 0) %>%
  pull(datetime)

vars <- unique(profile_data_grouped$variable)[-which(unique(profile_data_grouped$variable) == "chla")]

for(i in 1:length(vars)){
  
  var.df <- profile_data_grouped %>%
    filter(variable == vars[i]) 
  
  if(vars[i] == "oxygen") {
    var.df <- var.df %>%
      group_by(datetime) %>%
      filter(depth == max(depth, na.rm = TRUE)) %>%
      ungroup()
  } else if(vars[i] == "temperature") {
    var.df <- var.df %>%
      group_by(site_id, datetime, variable) %>%
      filter(depth <= 1) %>%
      summarize(observation = mean(observation, na.rm = TRUE)) %>%
      add_column(depth = 1) %>%
      ungroup()
  } else {
    var.df <- var.df %>%
      group_by(datetime) %>%
      filter(depth == 0) %>%
      ungroup()
  }
  
  var.df.final<-data.frame()
  
  for (j in 1:length(chla_dates)){
    
    nearest.date <-  var.df %>% 
      slice(which.min(abs(datetime - chla_dates[j])))
    
    if(abs(nearest.date$datetime[1] - chla_dates[j]) > 5) next
    
    nearest.date$chla_dates <- chla_dates[j]
    
    # Bind each of the data layers together.
    var.df.final = bind_rows(var.df.final, nearest.date)
    
  }
  
  if(i == 1){
    df.final <- var.df.final
  } else {
    df.final <- bind_rows(df.final, var.df.final)
  }
  
}

```

### Get met and flow data

1. wind 'sensor_repository_84749'.     
2. flow 'sensor_repository_00804'.     
3. light'sensor_repository_00962'.  

There aren't enough wind and light data in these files to be worth including in the module. Are these the right sensors to be using for the BAC and KEN sites? Also, how can I get data going further back than 2020 for these variables? For now, limiting everything to 2020-2024 so can use flow as a variable in the module.
```{r}
sensorCodes = c('sensor_repository_00804','sensor_repository_84749','sensor_repository_00962')
code_df = sensor_codes

filename = 'arms/wiski.csv'
bucket <- 'scevo-data'

fetchedData  <- data.frame()

for (i in sensorCodes) {
  object <- paste0("/arms/",i,".csv")
  print(object)
  #object <- paste0("/arms/",sensorCodes[i],".csv")
  rawSensorData <- aws.s3::s3read_using(FUN = utils::read.csv,
                                        check.names = FALSE,
                                        encoding = "UTF-8",
                                        # show_col_types = FALSE,
                                        # lazy = FALSE,
                                        # progress = FALSE,
                                        object = object,
                                        bucket = bucket,
                                        filename = basename(filename),
                                        opts = list(
                                          base_url = "projects.pawsey.org.au",
                                          region = "",
                                          key = "2f1a9d81bdf24a178b2bd18d530e959b",
                                          secret = "e062073c1faf488cb4209ba8de2eb483"))
  
  sensorData <- rawSensorData %>%
    mutate(date = as.POSIXct(Date*86400, 
                             origin=structure(-210866760000, 
                                              class=c("POSIXct", "POSIXt"),
                                              tzone="GMT"))) %>%
    mutate(date = format(round(date, units="hours"), format="%Y-%m-%d %H:%M:%S")) %>%
    arrange(date)
  
  sensorData$s_table_name <- i
  sensorData$variable <- code_df[which(code_df$s_table_name == i),'s_graph_value'][[1]]
  sensorData$unit <- code_df[which(code_df$s_table_name == i),'Unit'][[1]]
  sensorData <- sensorData %>%
    select(-QC, -Height, -Date) %>%
    dplyr::rename(observation = Data) %>%
    filter(!is.na(observation)) %>%
    select(date, s_table_name, variable, unit, observation)
  fetchedData <- rbind(fetchedData, sensorData)
}
```

### Data wrangling to aggregate high-frequency data
```{r}
dailyData <- fetchedData %>%
  mutate(day = date(date)) %>%
  group_by(day, s_table_name, variable, unit) %>%
  summarize(observation = mean(observation, na.rm = TRUE)) %>%
  ungroup() %>%
  dplyr::rename(datetime = day) %>%
  mutate(variable = ifelse(variable == "Flow (m3/s)","flow",
                           ifelse(variable == "Wind Speed avg","windspeed",
                                  ifelse(variable == "Solar Irradiance","solar_irradiance",NA))),
         obs_datetime = datetime) %>%
  filter(!is.na(variable)) %>%
  add_column(depth = NA,
             site_id = "SP") %>%
  select(site_id, datetime, depth, variable, observation, obs_datetime) %>%
  filter(datetime %in% chla_dates)

```

### Plot met and flow data  
**oofta where is all the solar irradiance and wind data?**
```{r}
ggplot(data = dailyData)+
  geom_point(aes(x = datetime, y = observation)) +
  facet_wrap(facets = vars(variable), scales = "free")
```

### Wrangle to get ALL the variables in one dataframe

This dataframe is used in Activity A, Objective 2 where students explore the data. It includes data gaps.
```{r}
chla_df <- profile_data_grouped %>%
  filter(variable == "chla" & year(datetime) %in% c(2020:2024)) %>%
  group_by(datetime) %>%
  filter(depth == 0) %>%
  ungroup() %>%
  mutate(chla_dates = datetime) %>%
  bind_rows(., df.final) %>%
  dplyr::rename(obs_datetime = datetime) %>%
  dplyr::rename(datetime = chla_dates) %>%
  bind_rows(., dailyData) 

module_data <- chla_df %>%
  filter(year(datetime) %in% c(2020:2024)) %>%
  select(-obs_datetime, -site_id, -depth) %>%
  pivot_wider(names_from = "variable", values_from = "observation")

write.csv(module_data, "/Users/marylofton/RProjects/module11_dev/app/data/kent.csv",row.names = FALSE)

```

### Plot final dataframe
```{r}
plot_dat <- chla_df %>%
  filter(year(datetime) %in% c(2020:2024))

ggplot(data = plot_dat)+
  geom_point(aes(x = datetime, y = observation, group = as.factor(depth), color = as.factor(depth)))+
  facet_wrap(facets = vars(variable), scales = "free_y")+
  theme_bw()
```

### Format for model fitting

This dataframe is used in Objectives 3 and 4 and I do gap-filling so the ARIMA model doesn't throw an error. The fable package is a bit picky about dates. Since we are just developing a model for educational purposes, for simplicity, I create a 5-year vector of weekly dates starting on the first sampling day of 2020 and use those dates for this dataframe (this means that in some cases the datetime of this dataframe is a few days off the actual sampling datetime, but it is always during the same week of the year). If two sampling events occur in the same week of the year, the values are averaged together. If there is a week of the year when sampling does not occur, those values are gap-filled using the 'updown' method.
```{r}
model_df <- chla_df %>%
  filter(year(datetime) %in% c(2020:2024)) %>%
  select(-obs_datetime, -site_id, -depth) %>%
  mutate(datetime = as.Date(datetime, origin = "1970-01-01")) %>%
  mutate(week = week(datetime),
         year = year(datetime)) %>%
  pivot_wider(names_from = "variable", values_from = "observation") 

year_week <- data.frame(year = rep(c(2020:2024), each = 52),
                        week = rep(c(1:52), times = 5),
                        week_dates = seq(from = as.Date("2020-01-07"), by = "week", length.out = 260))

model_df2 <- left_join(year_week, model_df, by = c("year","week")) %>%
  select(-solar_irradiance, -windspeed) %>% # just don't have enough data to make this worthwhile
  fill(chla:flow, .direction = "updown") %>%
  select(-datetime) %>%
  dplyr::rename(datetime = week_dates) %>%
  group_by(year, week) %>%
  summarize(across(datetime:flow, \(x) mean(x, na.rm = TRUE))) %>%
  mutate(datetime = as.Date(datetime, origin = "1970-01-01")) %>%
  ungroup()

write.csv(model_df2, "/Users/marylofton/RProjects/module11_dev/app/data/kent_mod.csv", row.names = FALSE)
```


### A bit more wranging to have a test standardized dataset for Activity B

This is NOT the dataset that students download for Kent St. Weir from Activity B (that one purposely has a formatting error introduced into the datetime column). The kent_std.csv dataset is only used by module developers for easy testing of Activity B/C functionality.
```{r}
# additional data wrangling to get standardized dataset for testing in Activity B

test_std_df <- model_df2 %>%
  select(-year, -week) %>%
  pivot_longer(chla:flow, names_to = "variable", values_to = "observation") %>%
  mutate(site_id = "kent") %>%
  select(site_id, datetime, variable, observation) 

write.csv(test_std_df, "/Users/marylofton/RProjects/module11_dev/app/data/kent_std.csv", row.names = FALSE)

test_dup_df <- test_std_df %>%
  mutate(datetime = ifelse((year(datetime) == 2020 & week(datetime) == 4), datetime + 1, datetime)) %>%
 mutate(datetime = as.Date(datetime, origin = "1970-01-01")) 

wid_dup <- test_dup_df %>%
  pivot_wider(names_from = "variable", values_from = "observation")

wid_dup[5,] <- wid_dup[6,]

tsb_dup <- tsibble(wid_dup)

duplicates(tsb_dup)
gap_count <- count_gaps(tsb_dup)
sum(gap_count$.n)

pivot_check <- test_dup_df %>%
      group_by(variable) %>%
      summarize(n = n(),
                distinct_dates = length(unique(datetime)),
                diff = n - distinct_dates)

any(pivot_check$diff) > 0

write.csv(test_dup_df, "/Users/marylofton/RProjects/module11_dev/app/data/kent_test.csv", row.names = FALSE)
```



